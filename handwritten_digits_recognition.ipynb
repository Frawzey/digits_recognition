{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "handwritten_digits_recognition.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "r6fF6UHD6Hjs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Simple Handwritten Digit Recognition Neural Network\n",
        "\n",
        "## Brief introduction\n",
        "\n",
        "#### A neural network consists of a input layer, hidden layer (middle layer) and the output layer. The input layer takes in the input (images, files, audio, video etc), passes it to the hidden layer where come processing/learning is done and passed to the output layer for results.\n",
        "#####Take a moment to think about this: let's assume you are in a group of 3 friends and you want to tell your 3rd friend you love her. You are the first friend, your second friend, Jay is the middleman or the channel of communication between you and your 3rd friend, Lola.\n",
        "#####It means you are the input node(s), Jay is the hidden/middle node and Lola is the output node. Let's say you made a casual whisper to Jay to inform Lola you love her. Jay is reluctant but goes on to say it to Lola.\n",
        "#####Its easy for Lola to smile and discard it - meaning the output was not strong enough. Let's assume you call Jay to a corner and tell him with all seriousness that you love Lola and that he should tell Lola the same say you told him.\n",
        "#####Jay did exactly what you told him. Lola would likely take it more seriously put that into consideration. She might even reply telling you she loves you too. The words you said are the input, the whispered joking word can be said to have a small weight - its not really serious.\n",
        "#####While the seriousness you added to the corner talk had more weight in shaping Lola's response. This is explains the basics of how neural network works. Its takes the product of the input from each layer multiplies it by the weight to give an output.\n",
        "#####Now, let's assume Jay told Lola that you \"like\" her instead of \"love\". That's an error. You had it mind that he would tell Lola with all seriousness that you love her but he didn't. What you had in mind was an intended output (target) while what Jay said was the actual output.\n",
        "#####Mathematically we can calculate this as:\n",
        "#####output error = intended output - actual output.\n",
        "#####We can moderate this output error by including a learning rate. This learning rate is a figure that we'd multiply with the output error to reduce it so that when next we tell Jay to speak with Lola, the error is minimized. The learning rate is usually a small number.\n",
        "#####Before we wrap up, lets assume that Lola has a level or threshold that must be met before she takes people's word into consideration. I mean, there's a level of 'trust' that must met for her to \"believe\" the speaker.\n",
        "#####Mathematically, the threshold that measures the level of 'trust' is called the activation/sigmoid/logistic function.\n",
        "#####So it means, Jay must meet that level for her to believe him.\n",
        "#####Trust me, we all have it. So it means even for you & Jay, there's also a level of seriousness or trust that must be overcome before you can pass your message across.\n",
        "#####Putting this analogy to the neural network, all layers (input, hidden & output layers) have an activation function that must be overcome for a successful message transfer.\n",
        "#####Finally, you can improve the output by increasing the number of times you relay your message to Jay as he also speaks to Lola. If you notice that Lola's output was way below our intended result, you can call Jay to the corner again to tell him the same message to tell.\n",
        "#####Hoping that it would minimize Jay's error and improve Lola's output.\n",
        "#####The number of times we relay our message (input) is called an epoch. We can have 5 epochs so as to improve our output. And the process of relaying the message is called training.\n",
        "#####Now lets make a recap:\n",
        "#####Input layer: is the entrance of the neural network\n",
        "#####Hidden layer: where communication (learning) happens\n",
        "#####Output layer: where the results happen\n",
        "#####Output error: intended output - actual error\n",
        "#####Learning rate: moderating factor used to minimize the error\n",
        "#####Activation function: threshold that must be overcome for an input to move to the next layer. It takes in the input at every layer.\n",
        "#####Epoch: number of times a training is carried out.\n",
        "#####Putting it all together, a neural network combines the inputs, learning rate, weights, errors and the activation function to give us the output.\n",
        "##### Now to the codes!"
      ]
    },
    {
      "metadata": {
        "id": "TfaEve8JvyLf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# import libraries\n",
        "\n",
        "import numpy as np\n",
        "import scipy.special"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "P16seHfF6hbJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##### We create a class called `neuralNetwork` with 3 functions: init, train, query\n",
        "##### `init` is to initialize the class and define our variables: inputnodes, outputnodes, hidden nodes & learning rate"
      ]
    },
    {
      "metadata": {
        "id": "rI7CGPCLv3uq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class neuralNetwork():\n",
        "    def __init__(self, inodes, onodes, hnodes, lrate):\n",
        "        # creating the input, output, hidden nodes & learning rate\n",
        "        self.inodes = inodes\n",
        "        self.onodes = onodes\n",
        "        self.hnodes = hnodes\n",
        "        self.lrate = lrate\n",
        "\n",
        "        # randomly creates weight to the hidden layer from the input layer\n",
        "        self.whi = np.random.normal(0.0, pow(self.hnodes, -0.5), (self.inodes, self.hnodes))\n",
        "        # randomly creates weight to the output layer from the hidden layer\n",
        "        self.who = np.random.normal(0.0, pow(self.hnodes, -0.5), (self.hnodes, self.onodes))\n",
        "        # activation function\n",
        "        self.activ_funct = lambda x: scipy.special.expit(x)\n",
        "        pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Z37h4JL06zNU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##### Next, the `train` function takes in the input (input_list) and the intended output (targets_lists) and then use the activation function to calculate the output. This output will be included in the multiplication with the learning rate & errors."
      ]
    },
    {
      "metadata": {
        "id": "hU_dKCzfv8ex",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "    def train(self, inputs_lists, targets_lists):\n",
        "        # converts inputs and targets to array\n",
        "        inputs = np.array(inputs_lists, ndmin=2).T\n",
        "        targets = np.array(targets_lists, ndmin=2).T\n",
        "\n",
        "        # calculate the input & output for the hidden layer - np.dot is used for multiplying matrices\n",
        "        hidden_inputs = np.dot(self.whi, inputs)\n",
        "        hidden_outputs = self.activ_funct(hidden_inputs)\n",
        "\n",
        "        # calculate the input & output for the final layer\n",
        "        final_inputs = np.dot(self.who, hidden_outputs)\n",
        "        final_outputs = self.activ_funct(final_inputs)\n",
        "\n",
        "        # calculating the output errors\n",
        "        output_errors = targets - final_outputs\n",
        "\n",
        "        # calculating errors at the hidden layers from the outputs\n",
        "        hidden_errors = np.dot(self.who.T, output_errors)\n",
        "\n",
        "        # calculating weight change from the output to the hidden layer\n",
        "        self.who += self.lrate * np.dot(final_outputs * output_errors * (1.0 - final_outputs), np.transpose(hidden_outputs))\n",
        "        # calculating weight change from the input to the hidden layer\n",
        "        self.whi += self.lrate * np.dot(hidden_outputs * hidden_errors * (1.0 - hidden_outputs), np.transpose(inputs))\n",
        "        pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5TfiP1hM7E5n",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##### the `query` function is simply to get our output."
      ]
    },
    {
      "metadata": {
        "id": "9Ryn903OwEic",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "    def query(self, inputs_lists):\n",
        "        # converts list to 2D array\n",
        "        inputs = np.array(inputs_lists, ndmin=2).T\n",
        "\n",
        "        # finding the input & output to the hidden layers\n",
        "        hidden_inputs = np.dot(self.whi, inputs)\n",
        "        hidden_outputs = self.activ_funct(hidden_inputs)\n",
        "\n",
        "        # finding the input & output to the hidden layers\n",
        "        final_inputs = np.dot(self.who, hidden_outputs)\n",
        "        final_outputs = self.activ_funct(final_inputs)\n",
        "\n",
        "        return final_outputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "phkENV4Q7Uls",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##### Now lets train the network with a real data set what contains images of handwritten digits."
      ]
    },
    {
      "metadata": {
        "id": "XWP8ikPS3USj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# ---------- Training the network ----------\n",
        "# provide values for the nodes & learning rate\n",
        "\n",
        "inodes = 784\n",
        "hnodes = 100\n",
        "onodes = 10\n",
        "lrate = 0.3\n",
        "\n",
        "# create instance for the neural network\n",
        "n = neuralNetwork(inodes, onodes, hnodes, lrate)\n",
        "\n",
        "# function that reads file\n",
        "def convert_file(csvfile):\n",
        "    # reads the csv file\n",
        "    f = open(csvfile, \"r\")\n",
        "    inputfile = f.readlines()\n",
        "    f.close()\n",
        "    return inputfile\n",
        "\n",
        "\n",
        "trainingFile = convert_file(\"mnist_train_100.csv\") \n",
        "\n",
        "for item in trainingFile:\n",
        "    split_training = item.split(',')\n",
        "    \n",
        "    # scaling the data i.e adjusting all values to be between 0.01 & 0.99\n",
        "    inputs = (np.asfarray(split_training[1:])/255 * 0.99) + 0.01\n",
        "    \n",
        "    # np.zeros convert all inputs to 0. Added 0.01 so that our output would not give zeros during multiplication\n",
        "    targets = np.zeros(onodes) + 0.01\n",
        "    targets[int(split_training[0])] = 0.99\n",
        "    \n",
        "    # calling the train function we defined earlier\n",
        "    n.train(inputs, targets)\n",
        "pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hbqS9sn77loz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#####Next is to test our results"
      ]
    },
    {
      "metadata": {
        "id": "dwCubqi57pQv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# ------ Testing the network -------\n",
        "\n",
        "# load the test data\n",
        "testingFile = convert_file(\"mnist_test_10.csv\")\n",
        "\n",
        "for item in testingFile:\n",
        "    split_test = item.split(',')\n",
        "    \n",
        "    # scaling the data i.e adjusting all values to be between 0.01 & 0.99\n",
        "    inputs = (np.asfarray(split_test[1:])/255 * 0.99) + 0.01\n",
        "    \n",
        "    # setting the input as 'correct label'\n",
        "    correct_label = int(split_test[0])\n",
        "    print(correct_label, \"correct label\")\n",
        "    \n",
        "    # calling the query function we defined earlier\n",
        "    outputs = n.query(inputs)\n",
        "    \n",
        "    # setting the output as 'label'\n",
        "    label = np.argmax(outputs)\n",
        "    print(label, \"network's result\")\n",
        "    \n",
        "    # creates a scorecard to measure results.\n",
        "    scorecard = []\n",
        "    if(label == correct_label):\n",
        "        scorecard.append(1)\n",
        "    else:\n",
        "        scorecard.append(0)\n",
        "    print(scorecard)\n",
        "pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6JdqGBLm7-SN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This notebook aims to create an understand of how the neural network works. It doesn't answer all questions on neural network but gives a beginner the nudge to learn more and dive deep.\n",
        "\n",
        "Thanks."
      ]
    }
  ]
}